{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni7wuN9XdZq5"
      },
      "outputs": [],
      "source": [
        "Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "*   A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used primarily for classification and regression tasks.\n",
        "*   Its main objective is to find the optimal decision boundary, called a hyperplane, that maximally separates data points of different classes in an N-dimensional space.\n",
        "\n"
      ],
      "metadata": {
        "id": "KDYPxMQLdhLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "*   A Hard Margin SVM requires a perfect, linear separation between classes, allowing absolutely no data points within the margin or on the wrong side of the decision boundary. This approach is only feasible for perfectly linearly separable data.\n",
        "\n",
        "*  A Soft Margin SVM, used in almost all real-world applications, is more flexible. It allows for a small degree of misclassification or points within the margin (called slack), balancing the goal of a wide margin with the goal of correctly classifying most data points.\n",
        "Key Distinctions\n",
        "\n"
      ],
      "metadata": {
        "id": "YZuHWJ_3eIhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "*   The mathematical intuition behind the Support Vector Machine (SVM) algorithm revolves around constrained optimization to find the hyperplane that creates the largest possible margin between different classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hq7sTiupefzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "*   Lagrange multipliers are a mathematical technique central to solving the Support Vector Machine (SVM) optimization problem.\n",
        "*   Their role is to transform the original constrained optimization problem (the \"primal\" problem) into an unconstrained one known as the \"dual\" problem, which offers key computational and practical advantages for SVMs.\n",
        "\n"
      ],
      "metadata": {
        "id": "8mJXMiwVfk2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are Support Vectors in SVM?\n",
        "\n",
        "*   Support vectors are the most critical data points in the Support Vector Machine (SVM) algorithm. They are the training examples that lie closest to the decision boundary (the hyperplane) and are the only points that actually determine the position and orientation of that boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "i1fMXKywgKbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "*   A Support Vector Classifier (SVC) is the specific implementation of the broader Support Vector Machine (SVM) framework designed purely for classification tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "F1TzE8S5gZqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "*   A Support Vector Regressor (SVR) is an application of the Support Vector Machine (SVM) framework adapted to solve regression problems, where the goal is to predict continuous numerical values rather than discrete classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "m6SjJk77gveR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?\n",
        "\n",
        "*   The Kernel Trick is a clever and computationally efficient mathematical technique that enables Support Vector Machines (SVMs) to find non-linear decision boundaries.\n",
        "\n"
      ],
      "metadata": {
        "id": "VQT4VxNjhCMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Compare Linear Kernel, Polynomial Kernel, and RBF Kernel?\n",
        "\n",
        " Decision Boundary\n",
        "\n",
        "*   \tLinear Kernel-\tStraight hyperplane\n",
        "\n",
        "*   polynominal Kernel- Curved/Non-linear\n",
        "*   RBF (Gaussian)- Complex/Non-linear\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mu5ERoiFhRt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM?\n",
        "\n",
        "*   The C parameter is one of the most critical hyperparameters in a Support Vector Machine (SVM) model, specifically within the Soft Margin formulation (used in most real-world scenarios).\n",
        "*   The C parameter directly controls the trade-off between maximizing the margin size and minimizing the classification error (misclassifications) on the training data. It is essentially a regularization parameter that dictates how strictly the model enforces perfect separation.\n",
        "\n"
      ],
      "metadata": {
        "id": "LGbRhhRAj0_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "*  The gamma parameter in an RBF (Radial Basis Function) Kernel SVM determines the reach or influence of a single training example on the decision boundary. It controls the flexibility and complexity of the model's decision boundary.\n",
        "* Intuitively, gamma defines the \"similarity radius\" or the inverse of the spread of the RBF kernel function (a Gaussian function) used to project the data into a higher dimension.  \n",
        "\n"
      ],
      "metadata": {
        "id": "6VGwuxCXkQph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "*   The classifier predicts the probability of a data point belonging to a certain class based on the probabilities of the features present in that data point.\n",
        "*   The algorithm is called \"Naïve\" because it makes a highly simplistic and often unrealistic assumption about the relationship between the input features: it assumes that all features are conditionally independent of each other, given the class label.\n",
        "\n"
      ],
      "metadata": {
        "id": "YSrSTukhlefD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What is Bayes’ Theorem?\n",
        "\n",
        "*   Bayes' Theorem is a fundamental mathematical formula in probability theory that describes how to update the probability of a hypothesis or event when given new, relevant evidence or information. It provides a rigorous way to reverse conditional probabilities and revise initial beliefs.\n",
        "\n"
      ],
      "metadata": {
        "id": "xghfx7BAmSpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?\n",
        "\n",
        "*   Gaussian Naïve Bayes is the variant for generic, continuous data. It assumes your numerical features (like height, weight, or age) are normally distributed within each class, using the mean and variance to calculate probabilities.\n",
        "*   Multinomial Naïve Bayes is the standard for text classification. It models the data based on the frequency or count of discrete events (like how many times a specific word appears in a document).ernoulli Naïve Bayes is similar to\n",
        "\n",
        "*   Bernoulli Naïve Bayes is similar to Multinomial but  strictly binary. It cares only about whether a feature is present or absent (a 0 or 1 value), not how often it appears. It explicitly penalizes the absence of a feature that is part of the model's vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kPy4hP0gmjSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "*   You should use Gaussian Naïve Bayes over the Multinomial or Bernoulli variants primarily when your input features are continuous, real-valued numbers and you can reasonably assume that these features follow a normal (Gaussian) distribution within each class.\n",
        "\n"
      ],
      "metadata": {
        "id": "TgzMkXMDn0Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "*   he Prior Probability Assumption (Bayes' Theorem Requirement)\n",
        "The algorithm uses the prior probability of a class to inform the overall prediction. It assumes the class frequencies observed in the training data are representative of the general population or the real world.  \n",
        "*   In Practice: If your training data has 90% spam emails and 10% legitimate emails, the model assumes any new incoming email is likely to be spam by default, before even looking at the email's content.\n",
        "\n"
      ],
      "metadata": {
        "id": "pWkVFaj3oUc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "*   Advantages :- Fast Training and Prediction: Naïve Bayes is computationally inexpensive. It requires only a single pass through the training data to calculate the necessary probabilities (means, variances, or counts), making it very fast to train. Predictions are also made rapidly.\n",
        "Works Well with High-Dimensional Data: It handles datasets with a large number of features efficiently. Because it makes the independence assumption, the model complexity doesn't increase as dramatically with the number of dimensions as some other algorithms.\n",
        "*   Disadvantages :- Zero Frequency Problem: If a category in the test data was never observed during training for a given class, the model assigns a zero probability, which ruins the prediction entirely. This requires mitigation using techniques like Laplace smoothing.\n",
        "Poor Probability Estimates: While it often produces good classifications, the actual probability scores it outputs (e.g., a 70% chance of spam) are often less reliable or less calibrated than those from other models like Logistic Regression or SVMs.\n",
        "\n"
      ],
      "metadata": {
        "id": "zUYs-1OVpDyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "*   Naïve Bayes is considered a superior and highly effective choice for text classification tasks (such as spam detection, sentiment analysis, and topic labeling) for several specific reasons related to the nature of text data and the algorithm's structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "488Wexbup0xk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Naïve Bayes for classification tasks?\n",
        "\n",
        "*   SVM and Naïve Bayes are both popular classification algorithms that approach the problem of classification from fundamentally different perspectives. SVM is a discriminative model, focusing on defining the decision boundary, while Naïve Bayes is a generative model, focusing on the probability distribution of the data .\n",
        "\n"
      ],
      "metadata": {
        "id": "zCSWASntqKmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "*  Laplace smoothing is a technique used in the Naïve Bayes classifier to address the \"zero-frequency problem,\" which can otherwise cause predictions to fail completely when a specific feature or feature value is missing from the training data for a certain class.\n",
        "\n"
      ],
      "metadata": {
        "id": "vyh0Gtk0qigF"
      }
    }
  ]
}